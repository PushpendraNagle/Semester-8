# -*- coding: utf-8 -*-
"""NLP_assign1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uucnx5j1xrAFnKtdo0-I3x9CCh-J2ZVu
"""

#Importing libraries
import nltk, re, pprint
import numpy as np
import pandas as pd
import requests
import matplotlib.pyplot as plt
import seaborn as sns
import pprint, time
import random
import json
import string
from sklearn.model_selection import train_test_split
from nltk.tokenize import word_tokenize

print (repr(string))

with open('penn-data.json', 'r') as infile:
        json_data = json.load(infile)
print (repr(string))

print(json_data[:5])
print (repr(string))

unique_tags = set()
tag_set = {'JJ', 'NNP', 'CD', 'JJS', 'FW', 'MD', 'VBP', 'NNPS', 'VBZ', 'WDT', 'PRP$', 'VBG', 'NNS', 'VBN', 'SYM', 'RB', 'POS', 'RBS', 'WP$', 'UH', 'PDT', 'IN', 'TO', 'WP', 'EX', 'VBD', 'JJR', 'RBR', 'VB', 'CC', 'PRP', 'LS', 'NN', 'DT', 'WRB', 'RP'}
print(len(tag_set))
print(sorted(tag_set))
for i in range(len(json_data)):
  for j in range(len(json_data[i][1])):
    unique_tags.add(json_data[i][1][j])
print(len(unique_tags))
for _ in unique_tags:
  print(_)

train_set, test_set = train_test_split(json_data,test_size=0.2, random_state=777)
print (repr(string))

print(len(train_set))

print(len(test_set))

print(train_set[:5])

def diff(list1, list2):
    list2 = set(list2)
    return [item for item in list1 if item not in list2]
print (repr(string))

print(train_set[1][0])
x = train_set[1][0]
x = x.translate(str.maketrans('', '', string.punctuation))
print(x)
print(len(x.split(" ")))
print(train_set[1][1])
print (repr(string))

def get_frequency(tag_dict):
    result = 0

    for tag, frequency in tag_dict.items():
      #print(tag)
      result += frequency

    return result
print (repr(string))

initial_frequency = {}
initial_probability = {}
transition_frequency = {}
transition_probability = {}
emission_frequency = {}
emission_probability = {}

tags_dict = {}

all_tags = []
all_words = []

for i in range(len(train_set)):
  tags = []
  words = []
  sentence = train_set[i][0]
  sentence_tags = train_set[i][1]
  sentence = sentence.translate(str.maketrans('', '', string.punctuation))
  sentence = sentence.split(" ")
  initial_frequency[sentence_tags[0]] = initial_frequency.get(sentence_tags[0], 0) + 1
  for _ in sentence:
    words.append(_.lower())
  for _ in sentence_tags:
    tags.append(_)
    tags_dict[_] = tags_dict.get(_, 0) + 1

  for _ in words:
    all_words.append(_)
  for _ in tags:
    all_tags.append(_)

start_size = get_frequency(initial_frequency)
print (repr(string))
# for _ in all_tags:
#   print(_)

for tag, freq in tags_dict.items():
    if tag in initial_frequency.keys():
        initial_probability[tag] = initial_frequency[tag] / start_size
    else:
        initial_probability[tag] = 0.0 / start_size

for i in range(len(all_tags)):
    transition_frequency[all_tags[i]] = {}
    transition_probability[all_tags[i]] = {}
    emission_frequency[all_tags[i]] = {}
    emission_probability[all_tags[i]] = {}
print (repr(string))

# #### TRANSITION FREQUENCY AND PROBABILITY #####
for i in range(len(train_set)):
    sentence = train_set[i][0]
    sentence_tags = train_set[i][1]

    for j in range(len(sentence_tags)-1):
        word_tag = sentence_tags[j]

        for k, v in transition_frequency.items():
            if k == word_tag:
                transition_frequency[k].update({sentence_tags[j+1]: transition_frequency[k].get(
                    sentence_tags[j+1], 0) + 1})

for k, v in transition_frequency.items():
    sum = get_frequency(v)
    for k2, v2 in v.items():
        transition_probability[k][k2] = v2 / sum
print (repr(string))

# #### EMISSION FREQUENCY AND PROBABILITY #####
for i in range(len(all_words)):
    for k, v in emission_frequency.items():
        if k == all_tags[i]:
            emission_frequency[k].update({all_words[i]: emission_frequency[k].get(all_words[i], 0) + 1})

for k, v in emission_frequency.items():
    sum = get_frequency(v)
    for k2, v2 in v.items():
        emission_probability[k][k2] = v2 / sum
print (repr(string))

def get_once_word(word_dict):
    count = 0
    for k, v in word_dict.items():
        if v == 1:
            count += 1
    return count


all_test_words = []

once_word_of_tags = {}

# #### READ TEST DATA #####
for i in range(len(test_set)):
    sentence = test_set[i][0]
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    sentence = sentence.split(" ")
    sentence_tags = test_set[i][1]
    tags = []
    words = []

    for j in range(len(sentence_tags)):
        tags.append(sentence_tags[j])
        words.append(sentence[j].lower())

    for word in words:
        all_test_words.append(word)

for tag in emission_frequency.keys():
    once_word_of_tags[tag] = get_once_word(emission_frequency[tag])

# #### NUMBER OF UNKNOWN WORDS WHICH APPEARED IN TEST DATA #####
unk_words = len(diff(all_test_words, all_words))
print(unk_words)
print (repr(string))

# #### VITERBI ALGORITHM #####
def viterbi(observation, states, one_freq_words, start_prob, transition_prob, emission_prob):
    viterbi_matrix = {}
    backpointer = {}
    observation_length = len(observation)

    # Initial word given
    for tag in states:
        first_word = observation[0]
        if first_word in emission_prob[tag].keys():
            viterbi_matrix[0, tag] = start_prob[tag] * emission_prob[tag].get(first_word, 0)
            backpointer[0, tag] = None
        else:
            viterbi_matrix[0, tag] = start_prob[tag] * (one_freq_words[tag] / (unk_words * get_frequency(emission_frequency[tag])))
            backpointer[0, tag] = None

    # Examine the second word now, we already examined first word
    for time_step, word in enumerate(observation[1:], start=1):
        for tag in states:
            if word in emission_prob[tag].keys():
                p_emission = emission_prob[tag].get(word, 0)
            else:
                p_emission = one_freq_words[tag] / (unk_words * get_frequency(emission_frequency[tag]))

            # argmax for the viterbi and backpointer
            probability, state = max((viterbi_matrix[time_step - 1, prev_tag] * transition_prob[prev_tag].get(tag, 0),
                                          prev_tag) for prev_tag in tags)

            # max probability for the viterbi matrix
            viterbi_matrix[time_step, tag] = probability * p_emission

            # state of the max probability for the backpointer
            backpointer[time_step, tag] = state

    # Termination steps
    probability, state = max((viterbi_matrix[observation_length - 1, tag] * transition_prob[tag].get('Punc', 0), tag)
                             for tag in tags)
    viterbi_matrix[observation_length, 'Punc'] = probability
    backpointer[observation_length, 'Punc'] = state

    # Return backtrace path...
    backtrace_path = []
    previous_state = 'Punc'
    for index in range(observation_length, 0, -1):
        state = backpointer[index, previous_state]
        backtrace_path.append(state)
        previous_state = state

    # We are tracing back through the pointers, so the path is in reverse
    backtrace_path = list(reversed(backtrace_path))
    return backtrace_path
print (repr(string))

tag_accuracy = {}
tag_total = {}
def calculate_accuracy(real_tags, test_tags):
    total_tags = 0
    true = 0
    for i in range(len(test_tags)):
        if test_tags[i] == real_tags[i]:
            true += 1
            tag_accuracy[real_tags[i]] = tag_accuracy.get(real_tags[i],0) + 1
        total_tags += 1
        tag_total[real_tags[i]] = tag_total.get(real_tags[i],0) + 1
    return true, total_tags
print (repr(string))

print(test_set[0][0])
print (repr(string))

# #### CHECK THE TAGS IN TEST SENTENCES #####
f = open("output.txt", "w", encoding='utf-8')

acc_total = 0
acc_correct = 0
print (repr(string))

for i in range(len(test_set)):
    tags = []
    words = []
    #sentence_temp = test_set[i][0]
    #print (repr(string))
    sentence = test_set[i][0].translate(str.maketrans('', '', string.punctuation))
    actual_words = sentence.split(" ")
    sentence_tags = test_set[i][1]

    for j in range(len(sentence_tags)):
        tags.append(sentence_tags[j])
        words.append(actual_words[j].lower())

    viterbi_result = viterbi(words, tags_dict.keys(), once_word_of_tags, initial_probability, transition_probability,
                             emission_probability)

    temp = ""
    for j in range(len(actual_words)):
        temp += actual_words[j] + "/" + viterbi_result[j] + " "

    temp += "\n"

    f.write(temp)

    true, total = calculate_accuracy(tags, viterbi_result)

    acc_total += total
    acc_correct += true

temp = "Accuracy: " + str((acc_correct / acc_total) * 100)
print("Correct found tags:", acc_correct)
print(len(tag_accuracy))
for tag in tag_accuracy:
  #print(tag)
  print("Accuracy of " + str(tag) + " :" + str((tag_accuracy[tag]/tag_total[tag])*100))
print("Total words:", acc_total)
print(temp)

f.close()

with open('penn-data.json', 'r') as infile:
        json_data1 = json.load(infile)
print (repr(string))

N_set = {'NN', 'NNS', 'NNP', 'NNPS'}
V_set = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}
A_set = {'JJ', 'JJR', 'JJS', 'RB', 'RB', 'RBR', 'RBS', 'WRB'}
for i in range(len(json_data1)):
  for j in range(len(json_data1[i][1])):
    if json_data1[i][1][j] in N_set:
      json_data1[i][1][j] = 'N'
    elif json_data1[i][1][j] in V_set:
      json_data1[i][1][j] = 'V'
    elif json_data1[i][1][j] in A_set:
      json_data1[i][1][j] = 'A'
    else:
      json_data1[i][1][j] = 'O'

print(json_data1[0][1])

train_set, test_set = train_test_split(json_data1,test_size=0.2, random_state=777)

initial_frequency = {}
initial_probability = {}
transition_frequency = {}
transition_probability = {}
emission_frequency = {}
emission_probability = {}

tags_dict = {}

all_tags = []
all_words = []

for i in range(len(train_set)):
  tags = []
  words = []
  sentence = train_set[i][0]
  sentence_tags = train_set[i][1]
  sentence = sentence.translate(str.maketrans('', '', string.punctuation))
  sentence = sentence.split(" ")
  initial_frequency[sentence_tags[0]] = initial_frequency.get(sentence_tags[0], 0) + 1
  for _ in sentence:
    words.append(_.lower())
  for _ in sentence_tags:
    tags.append(_)
    tags_dict[_] = tags_dict.get(_, 0) + 1

  for _ in words:
    all_words.append(_)
  for _ in tags:
    all_tags.append(_)

start_size = get_frequency(initial_frequency)
print (repr(string))

for tag, freq in tags_dict.items():
    if tag in initial_frequency.keys():
        initial_probability[tag] = initial_frequency[tag] / start_size
    else:
        initial_probability[tag] = 0.0 / start_size

for i in range(len(all_tags)):
    transition_frequency[all_tags[i]] = {}
    transition_probability[all_tags[i]] = {}
    emission_frequency[all_tags[i]] = {}
    emission_probability[all_tags[i]] = {}
print (repr(string))

# #### TRANSITION FREQUENCY AND PROBABILITY #####
for i in range(len(train_set)):
    sentence = train_set[i][0]
    sentence_tags = train_set[i][1]

    for j in range(len(sentence_tags)-1):
        word_tag = sentence_tags[j]

        for k, v in transition_frequency.items():
            if k == word_tag:
                transition_frequency[k].update({sentence_tags[j+1]: transition_frequency[k].get(
                    sentence_tags[j+1], 0) + 1})

for k, v in transition_frequency.items():
    sum = get_frequency(v)
    for k2, v2 in v.items():
        transition_probability[k][k2] = v2 / sum
print (repr(string))

# #### EMISSION FREQUENCY AND PROBABILITY #####
for i in range(len(all_words)):
    for k, v in emission_frequency.items():
        if k == all_tags[i]:
            emission_frequency[k].update({all_words[i]: emission_frequency[k].get(all_words[i], 0) + 1})

for k, v in emission_frequency.items():
    sum = get_frequency(v)
    for k2, v2 in v.items():
        emission_probability[k][k2] = v2 / sum
print (repr(string))

# #### READ TEST DATA #####
for i in range(len(test_set)):
    sentence = test_set[i][0]
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    sentence = sentence.split(" ")
    sentence_tags = test_set[i][1]
    tags = []
    words = []

    for j in range(len(sentence_tags)):
        tags.append(sentence_tags[j])
        words.append(sentence[j].lower())

    for word in words:
        all_test_words.append(word)

for tag in emission_frequency.keys():
    once_word_of_tags[tag] = get_once_word(emission_frequency[tag])

# #### NUMBER OF UNKNOWN WORDS WHICH APPEARED IN TEST DATA #####
unk_words = len(diff(all_test_words, all_words))
print(unk_words)
print (repr(string))

tag_accuracy = {}
tag_total = {}
# #### CHECK THE TAGS IN TEST SENTENCES #####
f = open("output.txt", "w", encoding='utf-8')

acc_total = 0
acc_correct = 0
print (repr(string))

for i in range(len(test_set)):
    tags = []
    words = []
    #sentence_temp = test_set[i][0]
    #print (repr(string))
    sentence = test_set[i][0].translate(str.maketrans('', '', string.punctuation))
    actual_words = sentence.split(" ")
    sentence_tags = test_set[i][1]

    for j in range(len(sentence_tags)):
        tags.append(sentence_tags[j])
        words.append(actual_words[j].lower())

    viterbi_result = viterbi(words, tags_dict.keys(), once_word_of_tags, initial_probability, transition_probability,
                             emission_probability)

    temp = ""
    for j in range(len(actual_words)):
        temp += actual_words[j] + "/" + viterbi_result[j] + " "

    temp += "\n"

    f.write(temp)

    true, total = calculate_accuracy(tags, viterbi_result)

    acc_total += total
    acc_correct += true

temp = "Accuracy: " + str((acc_correct / acc_total) * 100)
print("Correct found tags:", acc_correct)
print(len(tag_accuracy))
for tag in tag_accuracy:
  #print(tag)
  print("Accuracy of " + str(tag) + " :" + str((tag_accuracy[tag]/tag_total[tag])*100))
print("Total words:", acc_total)
print(temp)

f.close()